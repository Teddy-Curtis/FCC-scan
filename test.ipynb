{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "import uproot\n",
    "\n",
    "\n",
    "data_direc = f\"/vols/cms/emc21/fccStudy/data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run directory: =======\n",
      "runs/e240_run/run1\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/e240_run/run1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def getRunLoc(directory, prefix=False):\n",
    "    # Get a directory for the run: There might have been previous\n",
    "    # runs in the same directory, if so we don't want to overwite\n",
    "    # -> automatically +1 to end of the run name\n",
    "    direcs = glob.glob(f\"{directory}/*/\")\n",
    "    runs = sorted(\n",
    "        [int(direc.split(\"/\")[-2].split(\"run\")[1]) for direc in direcs]\n",
    "    )\n",
    "    if len(runs) == 0:  # if no runs yet\n",
    "        run_name = f\"run1\"\n",
    "    else:  # if there are runs, add 1 to end for new run name\n",
    "        n = runs[-1] + 1\n",
    "        run_name = f\"run{n}\"\n",
    "\n",
    "    if prefix:\n",
    "        run_name = f\"{prefix}_{run_name}\"\n",
    "\n",
    "    run_loc = f\"{directory}/{run_name}\"\n",
    "    print(f\"======= Run directory: =======\")\n",
    "    print(run_loc)\n",
    "    print(f\"==============================\")\n",
    "    # Make the run_loc\n",
    "    # os.makedirs(run_loc, exist_ok=True)\n",
    "\n",
    "    return run_loc\n",
    "\n",
    "\n",
    "base_run_dir = \"runs/e240_run\"\n",
    "run_loc = getRunLoc(base_run_dir)\n",
    "\n",
    "run_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenFields(evs):\n",
    "    for field in ak.fields(evs):\n",
    "        if \"var\" in str(ak.type(evs[field])):\n",
    "            evs[field] = ak.flatten(evs[field])\n",
    "    return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeight(evs, xs, lumi):\n",
    "    n_samples = len(evs)\n",
    "    weight = xs * lumi / n_samples\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def getData(samples, cuts=None):\n",
    "\n",
    "    events = []\n",
    "    # First load in the signal samples\n",
    "    for sig_point, sig_dict in samples['signal'].items():\n",
    "        print(f\"Loading signal point: {sig_point}\")\n",
    "        files = sig_dict[\"files\"]\n",
    "        files = [f\"{samples['directory']}/{f}\" for f in files]\n",
    "\n",
    "        for file in tqdm(files):\n",
    "            print(f\"Loading file: {file.split('/')[-1]}\")\n",
    "            with uproot.open(file, num_workers=16) as f:\n",
    "                tree = f[\"events\"]\n",
    "                branches_to_load = list(tree.keys())\n",
    "                branches_to_load.remove(\"n_seljets\")\n",
    "                evs = tree.arrays(branches_to_load, library=\"ak\")\n",
    "\n",
    "                weight = getWeight(evs, sig_dict[\"xs\"], samples[\"Luminosity\"])\n",
    "                evs[\"weight\"] = ak.ones_like(evs.Zcand_m) * weight\n",
    "\n",
    "                if cuts is not None:\n",
    "                    evs = cuts(evs)\n",
    "\n",
    "                # I want to add some stuff here as well to make the data more useful\n",
    "                class_num = ak.ones_like(evs.Zcand_m)\n",
    "                # Get the BP number\n",
    "                id_num = int(sig_point.split(\"BP\")[1])\n",
    "                evs[\"id_num\"] = ak.ones_like(evs.Zcand_m) * id_num\n",
    "                # Also get the masses\n",
    "                mH = sig_dict[\"masses\"][0]\n",
    "                mA = sig_dict[\"masses\"][1]\n",
    "                evs[\"mH\"] = ak.ones_like(evs.Zcand_m) * mH\n",
    "                evs[\"mA\"] = ak.ones_like(evs.Zcand_m) * mA\n",
    "\n",
    "                process = sig_point\n",
    "                specific_proc = file.split(\"_\")[-1].split(\".root\")[0]\n",
    "\n",
    "                evs[\"process\"] = [process] * len(evs.Zcand_m)\n",
    "                evs[\"specific_proc\"] = [specific_proc] * len(evs.Zcand_m)\n",
    "                evs[\"class\"] = class_num\n",
    "\n",
    "                # flatten all fields\n",
    "                evs = flattenFields(evs)\n",
    "                # Convert all to float32\n",
    "                evs = ak.values_astype(evs, \"float32\")\n",
    "\n",
    "                events.append(evs)\n",
    "\n",
    "    # Now load in the background samples\n",
    "    for proc, proc_dict in samples['backgrounds'].items():\n",
    "        print(f\"Loading background process: {proc}\")\n",
    "        files = proc_dict[\"files\"]\n",
    "        files = [f\"{samples['directory']}/{f}\" for f in files]\n",
    "\n",
    "        for file in tqdm(files):\n",
    "            print(f\"Loading file: {file.split('/')[-1]}\")\n",
    "            with uproot.open(file, num_workers=16) as f:\n",
    "                tree = f[\"events\"]\n",
    "                branches_to_load = list(tree.keys())\n",
    "                branches_to_load.remove(\"n_seljets\")\n",
    "                evs = tree.arrays(branches_to_load, library=\"ak\")\n",
    "\n",
    "                weight = getWeight(evs, sig_dict[\"xs\"], samples[\"Luminosity\"])\n",
    "                evs[\"weight\"] = ak.ones_like(evs.Zcand_m) * weight\n",
    "\n",
    "                if cuts is not None:\n",
    "                    evs = cuts(evs)\n",
    "\n",
    "                # I want to add some stuff here as well to make the data more useful\n",
    "                class_num = ak.zeros_like(evs.Zcand_m)\n",
    "\n",
    "                evs[\"mH\"] = [-1] * len(evs)\n",
    "                evs[\"mA\"] = [-1] * len(evs)\n",
    "                evs[\"id_num\"] = [-1] * len(evs)\n",
    "\n",
    "                evs[\"process\"] = [proc] * len(evs.Zcand_m)\n",
    "                evs[\"specific_proc\"] = [proc] * len(evs.Zcand_m)\n",
    "                evs[\"class\"] = class_num\n",
    "\n",
    "                # flatten all fields\n",
    "                evs = flattenFields(evs)\n",
    "                # Convert all to float32\n",
    "                evs = ak.values_astype(evs, \"float32\")\n",
    "\n",
    "                events.append(evs)\n",
    "    \n",
    "    events = ak.concatenate(events)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"data_directory\" : \"data\",\n",
    "    \"backgrounds\" : {\n",
    "    \"wzp6_ee_mumu_ecm240\": {\n",
    "        \"files\" : [\"wzp6_ee_mumu_ecm240.root\"],\n",
    "        \"xs\": 5.288\n",
    "    },\n",
    "    \"wzp6_ee_tautau_ecm240\": {\n",
    "        \"files\" : [\"wzp6_ee_tautau_ecm240.root\"],\n",
    "        \"xs\": 4.668\n",
    "    }\n",
    "    },\n",
    "    \"signal\" : {\n",
    "        \"BP1\" : {\n",
    "            \"files\" : [\"e240_bp1_h2h2ll.root\", \"e240_bp1_h2h2llvv.root\"],\n",
    "            \"masses\" : [80, 150],\n",
    "            \"xs\": 0.0069\n",
    "        },\n",
    "        \"BP2\" : {\n",
    "            \"files\" : [\"e240_bp2_h2h2ll.root\", \"e240_bp2_h2h2llvv.root\"],\n",
    "            \"masses\" : [80, 160],\n",
    "            \"xs\": 0.005895\n",
    "        },\n",
    "    },\n",
    "    \"Luminosity\" : 500,\n",
    "    \"directory\" : data_direc,\n",
    "    \"test_size\" : 0.25 # e.g. 0.2 means 20% of data used for test set\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading signal point: BP1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: e240_bp1_h2h2ll.root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Cuts\n",
      "Sum before: 3.4393792152404785, Sum after: 3.1485276222229004, Fraction: 0.9154348373413086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:04<00:04,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: e240_bp1_h2h2llvv.root\n",
      "Applying Cuts\n",
      "Sum before: 3.448861598968506, Sum after: 3.1020712852478027, Fraction: 0.8994479179382324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading signal point: BP2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: e240_bp2_h2h2ll.root\n",
      "Applying Cuts\n",
      "Sum before: 2.946505069732666, Sum after: 2.9245266914367676, Fraction: 0.9925408363342285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: e240_bp2_h2h2llvv.root\n",
      "Applying Cuts\n",
      "Sum before: 2.94511079788208, Sum after: 2.6585686206817627, Fraction: 0.9027057886123657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading background process: wzp6_ee_mumu_ecm240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: wzp6_ee_mumu_ecm240.root\n",
      "Applying Cuts\n",
      "Sum before: 2.943448066711426, Sum after: 0.77073734998703, Fraction: 0.26184844970703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading background process: wzp6_ee_tautau_ecm240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: wzp6_ee_tautau_ecm240.root\n",
      "Applying Cuts\n",
      "Sum before: 2.974040985107422, Sum after: 1.8348617553710938, Fraction: 0.6169591546058655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.34s/it]\n"
     ]
    }
   ],
   "source": [
    "def applyCuts(evs):\n",
    "    print(f\"Applying Cuts\")\n",
    "    mask = (\n",
    "        (np.abs(evs.Zcand_pz) < 70)\n",
    "        & (evs.jet1_e < 0)\n",
    "        & (evs.n_photons == 0)\n",
    "        & (evs.MET_pt > 5)\n",
    "        & (evs.lep1_pt < 80)\n",
    "        & (evs.lep2_pt < 60)\n",
    "        & (evs.Zcand_povere > 0.1)\n",
    "    )\n",
    "    mask = ak.flatten(mask)\n",
    "    sum_before = ak.sum(evs.weight)\n",
    "    sum_after = ak.sum(evs[mask].weight)\n",
    "    print(\n",
    "        f\"Sum before: {sum_before}, Sum after: {sum_after}, Fraction: {sum_after/sum_before}\"\n",
    "    )\n",
    "    return evs[mask]\n",
    "\n",
    "\n",
    "data = getData(samples, applyCuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zcand_m        , mean = 2412.65\n",
      "Zcand_pt       , mean = 578.06\n",
      "Zcand_pz       , mean = 715.23\n",
      "Zcand_p        , mean = 1293.54\n",
      "Zcand_povere   , mean = 0.50\n",
      "Zcand_e        , mean = 3706.33\n",
      "Zcand_costheta , mean = 0.45\n",
      "Zcand_recoil_m , mean = 34162.81\n",
      "photon1_pt     , mean = 1.00\n",
      "photon1_eta    , mean = 24.37\n",
      "photon1_e      , mean = 1.00\n",
      "lep1_pt        , mean = 999.55\n",
      "lep1_eta       , mean = 0.70\n",
      "lep1_e         , mean = 1757.68\n",
      "lep1_charge    , mean = 1.00\n",
      "lep2_pt        , mean = 232.62\n",
      "lep2_eta       , mean = 1.04\n",
      "lep2_e         , mean = 460.34\n",
      "lep2_charge    , mean = 1.00\n",
      "lep_chargeprod , mean = 1.00\n",
      "jet1_pt        , mean = 1.00\n",
      "jet1_eta       , mean = 24.37\n",
      "jet1_e         , mean = 1.00\n",
      "jet2_pt        , mean = 1.00\n",
      "jet2_eta       , mean = 24.37\n",
      "jet2_e         , mean = 1.00\n",
      "cosDphiLep     , mean = 0.84\n",
      "cosThetaStar   , mean = 0.33\n",
      "cosThetaR      , mean = 0.33\n",
      "n_jets         , mean = 2.97\n",
      "MET_e          , mean = 1304.22\n",
      "MET_pt         , mean = 579.92\n",
      "MET_eta        , mean = 1.50\n",
      "MET_phi        , mean = 3.29\n",
      "n_photons      , mean = 0.00\n",
      "n_muons        , mean = 2.13\n",
      "n_electrons    , mean = 1.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "branches = ['Zcand_m',\n",
    " 'Zcand_pt',\n",
    " 'Zcand_pz',\n",
    " 'Zcand_p',\n",
    " 'Zcand_povere',\n",
    " 'Zcand_e',\n",
    " 'Zcand_costheta',\n",
    " 'Zcand_recoil_m',\n",
    " 'photon1_pt',\n",
    " 'photon1_eta',\n",
    " 'photon1_e',\n",
    " 'lep1_pt',\n",
    " 'lep1_eta',\n",
    " 'lep1_e',\n",
    " 'lep1_charge',\n",
    " 'lep2_pt',\n",
    " 'lep2_eta',\n",
    " 'lep2_e',\n",
    " 'lep2_charge',\n",
    " 'lep_chargeprod',\n",
    " 'jet1_pt',\n",
    " 'jet1_eta',\n",
    " 'jet1_e',\n",
    " 'jet2_pt',\n",
    " 'jet2_eta',\n",
    " 'jet2_e',\n",
    " 'cosDphiLep',\n",
    " 'cosThetaStar',\n",
    " 'cosThetaR',\n",
    " 'n_jets',\n",
    " 'MET_e',\n",
    " 'MET_pt',\n",
    " 'MET_eta',\n",
    " 'MET_phi',\n",
    " 'n_photons',\n",
    " 'n_muons',\n",
    " 'n_electrons']\n",
    "\n",
    "\n",
    "for b in branches:\n",
    "    print(f\"{b: <15}, mean = {np.mean(data[b]**2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " ...,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]\n",
       "-----------------------\n",
       "type: 1801498 * float32</pre>"
      ],
      "text/plain": [
       "<Array [-1, -1, -1, -1, -1, ..., -1, -1, -1, -1, -1] type='1801498 * float32'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.photon1_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.any(data.jet1_pt != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1e-05, 1.1e-05, 1.1e-05, 1.1e-05, ..., 2.68e-05, 2.68e-05, 2.68e-05]\n",
      "Process: BP1, Number of events: 6.24392557144165\n",
      "[1.65e-05, 1.65e-05, 1.65e-05, 1.65e-05, ..., 2.27e-05, 2.27e-05, 2.27e-05]\n",
      "Process: BP2, Number of events: 5.597738265991211\n",
      "[8.42e-06, 8.42e-06, 8.42e-06, 8.42e-06, ..., 8.42e-06, 8.42e-06, 8.42e-06]\n",
      "Process: wzp6_ee_mumu_ecm240, Number of events: 0.77073734998703\n",
      "[1.82e-06, 1.82e-06, 1.82e-06, 1.82e-06, ..., 1.82e-06, 1.82e-06, 1.82e-06]\n",
      "Process: wzp6_ee_tautau_ecm240, Number of events: 1.8348617553710938\n"
     ]
    }
   ],
   "source": [
    "procs = np.unique(data.process)\n",
    "for proc in procs:\n",
    "    weights = data[data.process == proc].weight\n",
    "    print(weights)\n",
    "    print(f\"Process: {proc}, Number of events: {np.sum(weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing train test split\n",
      "['BP1', 'BP2', 'wzp6_ee_mumu_ecm240', 'wzp6_ee_tautau_ecm240']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def consistentTrainTestSplit(\n",
    "    events, test_size=0.2, random_state=42, stratify_var=\"process\"\n",
    "):\n",
    "\n",
    "    print(\"Doing train test split\")\n",
    "    unique_strat_vars = list(np.unique(events[stratify_var]))\n",
    "    print(unique_strat_vars)\n",
    "\n",
    "    # Split using indexes as it is much faster\n",
    "    indexes = np.arange(len(events))\n",
    "\n",
    "    train_data_idxs, test_data_idxs = train_test_split(\n",
    "        indexes,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=events[stratify_var],\n",
    "    )\n",
    "\n",
    "    train_data = events[train_data_idxs]\n",
    "    test_data = events[test_data_idxs]\n",
    "\n",
    "    train_data[\"weight_scaled\"] = train_data[\"weight\"] / (1 - test_size)\n",
    "    test_data[\"weight_scaled\"] = test_data[\"weight\"] / test_size\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# Now split into train and test\n",
    "train_data, test_data = consistentTrainTestSplit(data, \n",
    "                                                 test_size=samples['test_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP1\n",
      "BP2\n",
      "Sum sig = 522.0, sum bkg = 522.0\n",
      "BP1\n",
      "BP2\n",
      "Sum sig = 174.0, sum bkg = 174.0\n"
     ]
    }
   ],
   "source": [
    "def normaliseWeights(events):\n",
    "    # Normalise so the signal process sum to the same weight\n",
    "    # Start by making all signal samples to sum to 1\n",
    "    sig_dat = events[events[\"class\"] == 1]\n",
    "    signal_procs = np.unique(sig_dat[\"process\"])\n",
    "\n",
    "    # Want sum of weights of each signal to equal 1, then I will reweight\n",
    "    # both such that the average weight = 0.001\n",
    "    for proc in signal_procs:\n",
    "        print(proc)\n",
    "        proc_data = events[events[\"process\"] == proc]\n",
    "        sum_weight = np.sum(proc_data[\"weight\"])\n",
    "\n",
    "        events[\"weight\"] = ak.where(\n",
    "            events[\"process\"] == proc,\n",
    "            events[\"weight\"] / sum_weight,\n",
    "            events[\"weight\"],\n",
    "        )\n",
    "\n",
    "    # now reweight so that the average weight of signal = 0.001\n",
    "    mean_sig = np.mean(events[events[\"class\"] == 1][\"weight\"])\n",
    "    ratio = 0.001 / mean_sig\n",
    "    events[\"weight\"] = events[\"weight\"] * ratio\n",
    "\n",
    "    # Now get the sum of signal and reweight background to that\n",
    "    sum_sig = np.sum(events[events[\"class\"] == 1][\"weight\"])\n",
    "    sum_bkg = np.sum(events[events[\"class\"] == 0][\"weight\"])\n",
    "    events[\"weight\"] = ak.where(\n",
    "        events[\"class\"] == 0,\n",
    "        events[\"weight\"] * (sum_sig / sum_bkg),\n",
    "        events[\"weight\"],\n",
    "    )\n",
    "\n",
    "    # Now check that the sum of signal and background are the same\n",
    "    sig = events[events[\"class\"] == 1]\n",
    "    bkg = events[events[\"class\"] == 0]\n",
    "    sum_sig = np.floor(np.sum(sig['weight']))\n",
    "    sum_bkg = np.floor(np.sum(bkg['weight']))\n",
    "    print(f\"Sum sig = {sum_sig}, sum bkg = {sum_bkg}\")\n",
    "\n",
    "    if abs(sum_sig - sum_bkg) > 2:\n",
    "        print(\"Sum sig does not equal sum background!\")\n",
    "\n",
    "    return events\n",
    "\n",
    "# sum(backgrounds) = sum(signal)\n",
    "train_data = normaliseWeights(train_data)\n",
    "test_data = normaliseWeights(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "def convertToNumpy(events: ak.Array, branches: list) -> np.array:\n",
    "    # if only 1 branch then don't do the view stuff below, as that returns\n",
    "    # garbage\n",
    "    if len(branches) == 1:\n",
    "        return ak.to_numpy(events[branches[0]])\n",
    "\n",
    "    numpy_data = (\n",
    "        ak.to_numpy(events[branches]).view(\"<f4\").reshape(-1, len(branches))\n",
    "    )\n",
    "    return numpy_data\n",
    "\n",
    "def applyScaler(evs, scaler, branches):\n",
    "    numpy_data = convertToNumpy(evs, branches)\n",
    "    trans_numpy_data = scaler.transform(numpy_data)\n",
    "    for i, br in enumerate(branches):\n",
    "        evs[br] = trans_numpy_data[:, i]\n",
    "\n",
    "    return evs\n",
    "\n",
    "\n",
    "def scaleFeatures(train_data, test_data, branches, run_loc=\".\"):\n",
    "    print(f\"Scaling features\")\n",
    "    scaler = StandardScaler()\n",
    "    numpy_data = convertToNumpy(train_data, branches)\n",
    "    scaler.fit(numpy_data)\n",
    "    del numpy_data\n",
    "\n",
    "    train_data = applyScaler(train_data, scaler, branches)\n",
    "    test_data = applyScaler(test_data, scaler, branches)\n",
    "\n",
    "    # Also scale the masses using the minmax scaler\n",
    "    # First select just the signal\n",
    "    sig_data = train_data[train_data[\"class\"] == 1]\n",
    "    mass_branches = [\"mH\", \"mA\"]\n",
    "    mass_scaler = MinMaxScaler()\n",
    "    numpy_data = convertToNumpy(sig_data, mass_branches)\n",
    "    mass_scaler.fit(numpy_data)\n",
    "\n",
    "    del numpy_data\n",
    "\n",
    "    train_data = applyScaler(train_data, mass_scaler, mass_branches)\n",
    "    test_data = applyScaler(test_data, mass_scaler, mass_branches)\n",
    "\n",
    "    # Now save them both\n",
    "    pickle.dump(scaler, open(f\"{run_loc}/scaler.pkl\", \"wb\"))\n",
    "    pickle.dump(mass_scaler, open(f\"{run_loc}/mass_scaler.pkl\", \"wb\"))\n",
    "\n",
    "    return train_data, test_data, scaler, mass_scaler\n",
    "\n",
    "\n",
    "branches = ['Zcand_m',\n",
    " 'Zcand_pt',\n",
    " 'Zcand_pz',\n",
    " 'Zcand_p',\n",
    " 'Zcand_povere',\n",
    " 'Zcand_e',\n",
    " 'Zcand_costheta',\n",
    " 'Zcand_recoil_m',\n",
    " 'photon1_pt',\n",
    " 'photon1_eta',\n",
    " 'photon1_e',\n",
    " 'lep1_pt',\n",
    " 'lep1_eta',\n",
    " 'lep1_e',\n",
    " 'lep1_charge',\n",
    " 'lep2_pt',\n",
    " 'lep2_eta',\n",
    " 'lep2_e',\n",
    " 'lep2_charge',\n",
    " 'lep_chargeprod',\n",
    " 'jet1_pt',\n",
    " 'jet1_eta',\n",
    " 'jet1_e',\n",
    " 'jet2_pt',\n",
    " 'jet2_eta',\n",
    " 'jet2_e',\n",
    " 'cosDphiLep',\n",
    " 'cosThetaStar',\n",
    " 'cosThetaR',\n",
    " 'n_jets',\n",
    " 'MET_e',\n",
    " 'MET_pt',\n",
    " 'MET_eta',\n",
    " 'MET_phi',\n",
    " 'n_photons',\n",
    " 'n_muons',\n",
    " 'n_electrons']\n",
    "\n",
    "run_loc = \"test\"\n",
    "\n",
    "# Now scale the features so that intput features have mean 0 and std 1\n",
    "# and that the masses are scaled to be between 0 and 1\n",
    "train_data, test_data, feat_scaler, mass_scaler = scaleFeatures(train_data, \n",
    "                                                                test_data, \n",
    "                                                                branches,\n",
    "                                                                run_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch: Zcand_m, mean: -0.0001683638099164325, std: 1.0015350967692098\n",
      "Branch: Zcand_pt, mean: 0.0029507338719469887, std: 1.0007397873190778\n",
      "Branch: Zcand_pz, mean: 0.0017798503417291145, std: 0.9995208866371187\n",
      "Branch: Zcand_p, mean: 0.0014613517869223563, std: 0.9994377833407256\n",
      "Branch: Zcand_povere, mean: 0.0018473654224777964, std: 1.0003683340286504\n",
      "Branch: Zcand_e, mean: 0.0008332639467110741, std: 1.00061710744284\n",
      "Branch: Zcand_costheta, mean: 0.0014219622753608105, std: 0.9994216638513411\n",
      "Branch: Zcand_recoil_m, mean: -0.0008649232193658063, std: 1.000430597637535\n",
      "Branch: photon1_pt, mean: 0.0, std: 0.0\n",
      "Branch: photon1_eta, mean: 0.0, std: 0.0\n",
      "Branch: photon1_e, mean: 0.0, std: 0.0\n",
      "Branch: lep1_pt, mean: -4.521254159131713e-05, std: 1.0006674143363432\n",
      "Branch: lep1_eta, mean: 0.0033283518617298084, std: 1.0024313125018913\n",
      "Branch: lep1_e, mean: 0.0018345988260650846, std: 1.0011671128722135\n",
      "Branch: lep1_charge, mean: 0.0012667559206303775, std: 0.9999991976643969\n",
      "Branch: lep2_pt, mean: -0.0018482660933597003, std: 0.9982792149582391\n",
      "Branch: lep2_eta, mean: -0.0003398855920305128, std: 1.0011569194134007\n",
      "Branch: lep2_e, mean: -0.0013103503699174299, std: 0.9986501871963993\n",
      "Branch: lep2_charge, mean: -0.0012249018287711628, std: 0.9999992498074736\n",
      "Branch: lep_chargeprod, mean: 5.662466481828438e-08, std: 0.9999792531847277\n",
      "Branch: jet1_pt, mean: 0.0, std: 0.0\n",
      "Branch: jet1_eta, mean: 0.0, std: 0.0\n",
      "Branch: jet1_e, mean: 0.0, std: 0.0\n",
      "Branch: jet2_pt, mean: 0.0, std: 0.0\n",
      "Branch: jet2_eta, mean: 0.0, std: 0.0\n",
      "Branch: jet2_e, mean: 0.0, std: 0.0\n",
      "Branch: cosDphiLep, mean: 0.001988629673839509, std: 1.0021887996105123\n",
      "Branch: cosThetaStar, mean: 0.004379711243234804, std: 0.9999841988371824\n",
      "Branch: cosThetaR, mean: 0.004820865966729115, std: 1.0009111188727133\n",
      "Branch: n_jets, mean: -0.0018524035417534, std: 0.9984414563937322\n",
      "Branch: MET_e, mean: 0.001380971586178185, std: 0.9992423411727309\n",
      "Branch: MET_pt, mean: 0.0027096663153968917, std: 1.000136064254719\n",
      "Branch: MET_eta, mean: -0.0016869379954638495, std: 0.9987548446563441\n",
      "Branch: MET_phi, mean: 0.0008184132444967736, std: 0.9985719524063327\n",
      "Branch: n_photons, mean: 0.0, std: 0.0\n",
      "Branch: n_muons, mean: 0.0019400460282663753, std: 0.9980089030155992\n",
      "Branch: n_electrons, mean: -0.001942893861235429, std: 0.998094374868715\n"
     ]
    }
   ],
   "source": [
    "for branch in branches:\n",
    "    bdata = test_data[branch]\n",
    "    print(f\"Branch: {branch}, mean: {np.mean(bdata)}, std: {np.std(bdata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CustomDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CustomDataset\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        train_branches,\n",
    "        feat_scaler,\n",
    "        mass_scaler,\n",
    "        mass_branches=[\"mH\", \"mA\"],\n",
    "    ):\n",
    "        print(\"Initialising CustomDataset\")\n",
    "        self.train_branches = train_branches\n",
    "        self.data = data\n",
    "\n",
    "        # Get the features and masses\n",
    "        self.features = convertToNumpy(data, train_branches)\n",
    "        self.masses = convertToNumpy(data, mass_branches)\n",
    "        labs = convertToNumpy(data, [\"class\"])\n",
    "        self.labels = np.reshape(labs, (len(labs), 1))\n",
    "        weights = convertToNumpy(data, [\"weight\"])\n",
    "        self.weight = np.reshape(weights, (len(weights), 1))\n",
    "\n",
    "        if not \"weight_scaled\" in ak.fields(data):\n",
    "            data[\"weight_scaled\"] = data[\"weight\"]\n",
    "\n",
    "        weights_scaled = convertToNumpy(data, [\"weight_scaled\"])\n",
    "        self.weight_scaled = np.reshape(weights_scaled, (len(weights_scaled), 1))\n",
    "\n",
    "        # Now find the unique masses\n",
    "        # Pick only the signal masses, not the default ones for the backgrounds\n",
    "        sig_masses = self.masses[labs == 1]\n",
    "        self.unique_masses = np.unique(sig_masses, axis=0)\n",
    "\n",
    "        # Also save the scalers so that we can convert between the real data\n",
    "        # and the converted data\n",
    "        self.feat_scaler = feat_scaler\n",
    "        self.mass_scaler = mass_scaler\n",
    "\n",
    "    def shuffleMasses(self):\n",
    "        # Want to find unique groupings of the mH,mA,mHch\n",
    "        # Pick masses randomly from unique_masses\n",
    "        choices = np.random.choice(\n",
    "            np.arange(len(self.unique_masses)), len(self.labels)\n",
    "        )\n",
    "\n",
    "        shuffled_masses = self.unique_masses[choices]\n",
    "\n",
    "        # Now mask so that the original signal masses are not changed\n",
    "        sig_mask = self.labels == 1\n",
    "        new_masses = np.where(sig_mask, self.masses, shuffled_masses)\n",
    "\n",
    "        self.masses = new_masses\n",
    "\n",
    "    def setAllMasses(self, masses_to_set):\n",
    "        # Set all backgrounds to the same mass\n",
    "        # e.g. masses_to_set = [80, 100, 160]\n",
    "        # Expand out to same size of data array\n",
    "        specific_masses = np.array([masses_to_set] * len(self.labels))\n",
    "\n",
    "        # Now only apply the background samples\n",
    "        sig_mask = self.labels == 1\n",
    "        new_masses = np.where(sig_mask, self.masses, specific_masses)\n",
    "\n",
    "        self.masses = new_masses\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        mass = self.masses[idx]\n",
    "        w = self.weight[idx]\n",
    "        wc = self.weight_scaled[idx]\n",
    "        # self.features = convertToNumpy(self.data[idx], self.train_branches)\n",
    "        # self.masses = convertToNumpy(self.data[idx], self.mass_branches)\n",
    "        # self.labels = convertToNumpy(self.data[idx], ['class'])\n",
    "        # self.weight = convertToNumpy(self.data[idx], ['weight'])\n",
    "\n",
    "        return x, y, mass, w, wc\n",
    "\n",
    "    def toOriginalArray(self):\n",
    "        # Want to change back the self.data features and masses\n",
    "        features_nump = convertToNumpy(self.data, self.train_branches)\n",
    "        features_nump = self.feat_scaler.inverse_transform(features_nump)\n",
    "\n",
    "        mass_nump = convertToNumpy(self.data, self.mass_branches)\n",
    "        mass_nump = self.mass_scaler.inverse_transform(mass_nump)\n",
    "\n",
    "        for i, field in enumerate(self.train_branches):\n",
    "            self.data[field] = features_nump[:, i]\n",
    "\n",
    "        for i, field in enumerate(self.mass_branches):\n",
    "            self.data[field] = mass_nump[:, i]\n",
    "\n",
    "        return self.data\n",
    "\n",
    "# # Now put these both into helper classes for training \n",
    "train_dataset = CustomDataset(train_data, branches, feat_scaler, mass_scaler)\n",
    "test_dataset = CustomDataset(test_data, branches, feat_scaler, mass_scaler)\n",
    "train_dataset.shuffleMasses()\n",
    "test_dataset.shuffleMasses()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
